alloy:
  configMap:
    # https://grafana.com/docs/alloy/latest/collect/logs-in-kubernetes/
    content: |
      // discovery.kubernetes allows you to find scrape targets from Kubernetes resources.
      // It watches cluster state and ensures targets are continually synced with what is currently running in your cluster.
      discovery.kubernetes "pod" {
        role = "pod"
        selectors {
          role = "pod"
          field = "spec.nodeName=" + coalesce(sys.env("HOSTNAME"), constants.hostname)
        }
      }

      // discovery.relabel rewrites the label set of the input targets by applying one or more relabeling rules.
      // If no rules are defined, then the input targets are exported as-is.
      discovery.relabel "pod_logs" {
        targets = discovery.kubernetes.pod.targets

        // Label creation - "namespace" field from "__meta_kubernetes_namespace"
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          action = "replace"
          target_label = "namespace"
        }

        // Label creation - "pod" field from "__meta_kubernetes_pod_name"
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          action = "replace"
          target_label = "pod"
        }

        // Label creation - "container" field from "__meta_kubernetes_pod_container_name"
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          action = "replace"
          target_label = "container"
        }

        // Label creation -  "app" field from "__meta_kubernetes_pod_label_app_kubernetes_io_name"
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          action = "replace"
          target_label = "app"
        }

        // Label creation - "container" field from "__meta_kubernetes_pod_uid" and "__meta_kubernetes_pod_container_name"
        // Concatenate values __meta_kubernetes_pod_uid/__meta_kubernetes_pod_container_name.log
        rule {
          source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
          action = "replace"
          target_label = "__path__"
          separator = "/"
          replacement = "/var/log/pods/*$1/*.log"
        }

        // Label creation -  "container_runtime" field from "__meta_kubernetes_pod_container_id"
        rule {
          source_labels = ["__meta_kubernetes_pod_container_id"]
          action = "replace"
          target_label = "container_runtime"
          regex = "^(\\S+):\\/\\/.+$"
          replacement = "$1"
        }

        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          action = "replace"
          target_label = "node"
        }
      
        // Drop self logs (to avoid infinite error log loop)
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          regex = "^alloy$"
          action = "drop"
        }
      }

      loki.source.file "pod_logs" {
        targets    = discovery.relabel.pod_logs.output
        forward_to = [loki.process.pod_logs.receiver]
      }

      // loki.process receives log entries from other Loki components, applies one or more processing stages,
      // and forwards the results to the list of receivers in the component's arguments.
      loki.process "pod_logs" {
        stage.static_labels {
          values = {
            cluster = "moto-a1",
            job = "pod-logs",
          }
        }
  
        forward_to = [loki.write.in_cluster.receiver, otelcol.receiver.loki.gcp_logs.receiver]
      }
      
      loki.write "in_cluster" {
        endpoint {
          url = "http://loki.monitor.svc.cluster.local:3100/loki/api/v1/push"
        }
      }
      
      otelcol.receiver.loki "gcp_logs" {
        output {
          logs = [otelcol.processor.memory_limiter.gcp_logs.input]
        }
      }
      
      otelcol.processor.memory_limiter "gcp_logs" {
        check_interval = "1s"
        limit = "500MiB"
      
        output {
          metrics = [otelcol.processor.batch.gcp_logs.input]
          logs = [otelcol.processor.batch.gcp_logs.input]
          traces = [otelcol.processor.batch.gcp_logs.input]
        }
      }
      
      otelcol.processor.batch "gcp_logs" {
        output {
          metrics = [otelcol.exporter.googlecloud.gcp_logs.input]
          logs = [otelcol.exporter.googlecloud.gcp_logs.input]
          traces = [otelcol.exporter.googlecloud.gcp_logs.input]
        }
      }
      
      otelcol.exporter.googlecloud "gcp_logs" {
        log {
          default_log_name = "opentelemetry.io/collector-exported-log"
        }
      }

  storagePath: /run/alloy
  extraArgs:
    - --feature.community-components.enabled=true
  extraEnv:
    - name: GOOGLE_APPLICATION_CREDENTIALS
      value: /etc/otelcol-contrib/key.json
  mounts:
    varlog: true
    extra:
      - name: alloy
        mountPath: /run/alloy
      - name: sa
        mountPath: /etc/otelcol-contrib/key.json
        subPath: key.json

crds:
  create: false

image:
  pullPolicy: Always

controller:
  type: daemonset
  tolerations:
    - operator: Exists
  volumes:
    extra:
      - name: alloy
        hostPath:
          path: /run/alloy-ds
      - name: sa
        secret:
          secretName: sa
